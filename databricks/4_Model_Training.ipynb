{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9d4319c-3d1c-4361-87a2-d2ecd8281237",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Machine Learning Model Training (Multilabel Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f746c4-b38a-4ff1-9331-91402b9dfcba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read dataset from checkpoint\n",
    "model_df = spark.read.parquet(\"/tmp/project/preprocessed_data\")\n",
    "model_df.cache()\n",
    "\n",
    "display(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1992f730-cee3-4aae-a012-98ab7ed8d02f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0268f8e-0d0d-4084-b6a2-554aa846b21f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#Parameters\n",
    "maxIter = 100\n",
    "regParam = 0.0             # regularization didn't improve performance\n",
    "elasticNetParam = 0.0      # regularization didn't improve performance\n",
    "\n",
    "# Split dataset\n",
    "train_data, test_data = model_df.randomSplit([0.9, 0.1], seed=32)\n",
    "train_data = train_data.cache()\n",
    "\n",
    "# Define Logistic Regression model\n",
    "lr = LogisticRegression(maxIter=maxIter, regParam=regParam, elasticNetParam=elasticNetParam)\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions = lr_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f011985a-b53a-441a-a43f-9380f3742b18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Preparing labels for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36848f43-23dd-4676-8276-a6da20e9c8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define a UDF to extract labels with probabilities greater than some threshold\n",
    "@udf(ArrayType(StringType()))\n",
    "def extract_labels(probabilities, threshold=0.2):\n",
    "    labels = [int(i) for i, prob in enumerate(probabilities) if prob > threshold]\n",
    "    return labels\n",
    "\n",
    "# Find predicted labels based on probability threshold\n",
    "predictions_df = predictions.withColumn(\"predicted_labels\", extract_labels(predictions.probability))\n",
    "\n",
    "# Create a label dataframe containing lists of true and predicted labels for comparison\n",
    "labels_df = predictions_df.select(\"true_labels\", \"predicted_labels\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Convert to Pandas dataframe to use with MultiLabelBinarizer\n",
    "model_pd = model_df.toPandas()\n",
    "predictions_pd = predictions_df.toPandas()\n",
    "\n",
    "# Learning classes and creating a class mapping\n",
    "mlb.fit(model_pd[\"true_labels\"].apply(lambda lst: [int(label) for label in lst]))\n",
    "\n",
    "# Finding labels as a binarized list of all labels for each post\n",
    "true_labels_mlb = mlb.transform(predictions_pd[\"true_labels\"].apply(lambda lst: [int(label) for label in lst]))\n",
    "predicted_labels_mlb = mlb.transform(predictions_pd[\"predicted_labels\"].apply(lambda lst: [int(label) for label in lst]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c25aaf94-391c-47f7-accf-1f3d68a1e0d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e23dae9-3e94-4e19-a082-c7088c47d65a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation method 2:\n",
    "Accuracy is not considered a good metric for multi-label classification. Hamming loss is another metric that suits this problem better.\n",
    "Small values of te Hamming loss metric (near zero) indicate good performance. Here, precision and recall are also used to give a broader \n",
    "perspective on the model performance.\n",
    "\"\"\"\n",
    "accuracy = accuracy_score(true_labels_mlb, predicted_labels_mlb)\n",
    "precision = precision_score(true_labels_mlb, predicted_labels_mlb, average='micro')\n",
    "recall = recall_score(true_labels_mlb, predicted_labels_mlb, average='micro')\n",
    "hamming_loss = hamming_loss(true_labels_mlb, predicted_labels_mlb)\n",
    "\n",
    "print(\"Accuracy on testing dataset:\", accuracy)\n",
    "print(\"Precision on testing dataset:\", precision)\n",
    "print(\"Recall on testing dataset:\", recall)\n",
    "print(\"Hamming Loss on testing dataset:\", hamming_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc82b97a-5d30-4ef2-b7a3-d32d892a082d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show true and predicted labels for visual comaprison\n",
    "display(labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a677fd6-6562-4bff-a626-4ee6a677c2a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model training on full dataset for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9715aa-604e-4b5a-a78a-118877b49bd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Read dataset from checkpoint\n",
    "model_df = spark.read.parquet(\"/tmp/project/preprocessed_data\")\n",
    "model_df = model_df.cache()\n",
    "\n",
    "#Parameters\n",
    "maxIter = 100\n",
    "regParam = 0.0\n",
    "elasticNetParam = 0.0\n",
    "\n",
    "# Define Logistic Regression model\n",
    "lr = LogisticRegression(maxIter=maxIter, regParam=regParam, elasticNetParam=elasticNetParam)\n",
    "lr_model = lr.fit(model_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6406bd2-cc97-426e-ac4d-d88b2d2efcf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save models temporarily for use in deployment notebook\n",
    "lr_model.save('/mnt/bd-project/Models/lr_model')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Model_Training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
