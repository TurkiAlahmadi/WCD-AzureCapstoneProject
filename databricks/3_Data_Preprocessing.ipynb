{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b56ef61-138d-4c8c-92fc-7548e7137d86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32087142-a7fb-4a47-ae35-23a5294dac63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03fc0c3-7e5a-4cb3-bf22-334059690db9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql.functions import udf, col, array_contains, concat, lit, monotonically_increasing_id, regexp_replace, lower, explode, split, collect_list, instr\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, IDF, Normalizer, CountVectorizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.stem import PorterStemmer\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Loading posts from data lake\n",
    "posts = spark.read.parquet(\"/tmp/project/posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7edf11a9-62eb-4a72-a51c-63d50526c02d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d10964-0f0f-46ef-8216-97bd15108b61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df = posts[['id', 'Body', 'Title', 'Tags']]\n",
    "df = df.filter(instr(col(\"Tags\"), \"<\") > 0)\n",
    "\n",
    "# Concatinate body and title to introduce highly relevant features\n",
    "df = df.withColumn(\"text\", concat(col(\"Title\"), lit(\" \"), col(\"Body\")))\n",
    "\n",
    "df = df.withColumn(\"text\", regexp_replace(\"text\", r\"\\W+\", \" \")) \\\n",
    "     .withColumn(\"text\", regexp_replace(\"text\", r\"\\b\\w\\b\", \"\")) \\\n",
    "     .withColumn(\"text\", regexp_replace(\"text\", \"_\", \" \")) \\\n",
    "     .withColumn(\"text\", regexp_replace(\"text\", r\"\\s+\", \" \")) \\\n",
    "     .withColumn(\"text\", lower(\"text\"))\n",
    "\n",
    "df = df.withColumn(\"Tags\", regexp_replace(\"Tags\", r\"[<>]\", \" \")) \\\n",
    "     .withColumn(\"Tags\", regexp_replace(\"Tags\", r\"\\s+\", \" \")) \\\n",
    "     .withColumn(\"Tags\", lower(\"Tags\"))\n",
    "\n",
    "df = df.withColumn(\"Tags\", split(df.Tags, \" \"))\n",
    "df = df.select(\"id\", \"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "\n",
    "df = df[df['tags'] != '']\n",
    "\n",
    "# data shuffling\n",
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6daf80a8-ac7c-4956-b13c-c4ca82a57266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Label encoder\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\", handleInvalid='skip')\n",
    "\n",
    "label_model = label_encoder.fit(df)\n",
    "labels = label_model.transform(df)\n",
    "\n",
    "# Find true labels\n",
    "temp = labels.withColumnRenamed(\"id\", \"id_dup\") \\\n",
    "     .groupBy(\"id_dup\").agg(collect_list(\"label\").alias(\"true_labels\"))\n",
    "\n",
    "true_labels = (labels.join(temp, labels.id == temp.id_dup, 'left_outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380dc770-90f5-42ec-b782-9b597ea41393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee534d0-6185-44f6-86b3-ee1a0b07f17f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(true_labels)\n",
    "\n",
    "# Removing stop words\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stop_words\")\n",
    "stopword = stopword_remover.transform(tokenized)\n",
    "\n",
    "# Stemming\n",
    "# Use stemming because of the computer science jargon. Words can have very specific meanings and usages\n",
    "stemmer_func = udf(lambda words: [PorterStemmer().stem(word) for word in words], ArrayType(StringType()))\n",
    "stemmed = stopword.withColumn(\"stemmed\", stemmer_func(col(\"stop_words\")))\n",
    "\n",
    "# Removing additional stop words (highly frequent words based on EDA)\n",
    "custom_stop_words = ['code', 'use', 'pre', 'get', 'want', 'like', 'thank', 'tri', 'work', 'way', 'need'] # Here\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol=\"stemmed\", outputCol=\"filtered\", stopWords = custom_stop_words)\n",
    "filtered = stopword_remover.transform(stemmed)\n",
    "\n",
    "# Count vectorizer\n",
    "# There are ~8000 unique words\n",
    "\n",
    "cv = CountVectorizer(vocabSize= 8000, inputCol=\"filtered\", outputCol='cv') \n",
    "cv_model = cv.fit(filtered)\n",
    "text_cv = cv_model.transform(filtered)\n",
    "\n",
    "# TF-IDF\n",
    "# Remove words that appeared only once\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq = 2)\n",
    "idf_model = idf.fit(text_cv)\n",
    "text_idf = idf_model.transform(text_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd290493-56b7-44d1-ba02-24fb9d3c9d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checkpoint for next phase\n",
    "text_idf.write.mode('overwrite').parquet(\"/tmp/project/preprocessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831c984e-43f8-4271-8955-0b9e6b9182c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save preprocessing models\n",
    "label_model.write().save('/mnt/bd-project/Models/stringindexer')\n",
    "cv_model.write().save('/mnt/bd-project/Models/cv_model')\n",
    "idf_model.write().save(\"/mnt/bd-project/Models/tfidf\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_Data_Preprocessing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
