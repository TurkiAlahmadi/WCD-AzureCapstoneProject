{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dedd7fc3-9bd9-48f5-bebd-300573c605af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c4acc5-2332-4a1b-bf36-c277cfcf6ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03fc0c3-7e5a-4cb3-bf22-334059690db9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql.functions import udf, col, concat, lit, monotonically_increasing_id, regexp_replace, lower, explode, split, collect_list, array_contains, instr\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer, CountVectorizer, StringIndexer, StringIndexerModel, IDFModel\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.stem import PorterStemmer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "\n",
    "# Loading files and models\n",
    "posts = spark.read.parquet('/mnt/bd-project/Landing/Posts/*')\n",
    "\n",
    "model = LogisticRegressionModel.load('/mnt/bd-project/Models/lr_model')\n",
    "counterVec = CountVectorizerModel.load('/mnt/bd-project/Models/cv_model')\n",
    "lables = StringIndexerModel.load('/mnt/bd-project/Models/stringindexer')\n",
    "idf_model = IDFModel.load('/mnt/bd-project/Models/tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f9c922-d615-48b1-88c8-83196da24a42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d10964-0f0f-46ef-8216-97bd15108b61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df = posts[['id', 'Body', 'Title']]\n",
    "df = df.withColumn(\"text\", concat(col(\"Title\"), lit(\" \"), col(\"Body\")))\n",
    "\n",
    "df = df.withColumn(\"text\", regexp_replace(\"text\", r\"\\W+\", \" \")) \\\n",
    "     .withColumn(\"text\", regexp_replace(\"text\", r\"\\b\\w\\b\", \"\")) \\\n",
    "     .withColumn(\"text\", regexp_replace(\"text\", \"_\", \" \")) \\\n",
    "     .withColumn(\"text\", regexp_replace(\"text\", r\"\\s+\", \" \")) \\\n",
    "     .withColumn(\"text\", lower(\"text\"))\n",
    "\n",
    "df = df[df['text'].isNotNull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee534d0-6185-44f6-86b3-ee1a0b07f17f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(df)\n",
    "\n",
    "# Removing stop words\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stop_words\")\n",
    "stopword = stopword_remover.transform(tokenized)\n",
    "\n",
    "# Stemming\n",
    "stemmer_func = udf(lambda words: [PorterStemmer().stem(word) for word in words], ArrayType(StringType()))\n",
    "stemmed = stopword.withColumn(\"stemmed\", stemmer_func(col(\"stop_words\")))\n",
    "\n",
    "# Removing additional stop words (highly frequent words based on EDA)\n",
    "custom_stop_words = ['code', 'use', 'pre', 'get', 'want', 'like', 'thank', 'tri', 'work', 'way', 'need']\n",
    "\n",
    "custom_remover = StopWordsRemover(inputCol=\"stemmed\", outputCol=\"filtered\", stopWords = custom_stop_words)\n",
    "filtered = custom_remover.transform(stemmed)\n",
    "\n",
    "# Count vectorizer\n",
    "text_cv = counterVec.transform(filtered)\n",
    "\n",
    "# Performing TF-IDF\n",
    "final = idf_model.transform(text_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004c1557-438d-4cc0-8e98-1e4c32c5246e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Label prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e929cb-587d-4a12-88f8-cc757d7d4d7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predicting labels of new posts\n",
    "final = final.cache()\n",
    "predictions = model.transform(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d992b4-b246-4cf3-a438-3244fe6895c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(ArrayType(StringType()))\n",
    "def extract_labels(probabilities, threshold=0.2):\n",
    "    labels = [int(i) for i, prob in enumerate(probabilities) if prob > threshold]\n",
    "    return labels\n",
    "\n",
    "output = predictions.withColumn(\"predicted_labels\", extract_labels(predictions.probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f160460-3d1f-42ff-9306-f0dfd1496c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexer = lables\n",
    "exploded = output.withColumn(\"value\", explode(\"predicted_labels\"))\n",
    "numeric = exploded.withColumn(\"indexed_value\", col(\"value\").cast(\"integer\"))\n",
    "\n",
    "i2s = IndexToString(inputCol=\"indexed_value\", outputCol=\"temp_decoder\", labels=indexer.labels)\n",
    "temp_decoded = i2s.transform(numeric)\n",
    "\n",
    "temp = temp_decoded.withColumnRenamed(\"Body\", \"Body_two\") \\\n",
    "    .groupBy(\"Body_two\").agg(collect_list(\"temp_decoder\").alias(\"labels_decoder\"))\n",
    "\n",
    "output = output.join(temp, output.Body == temp.Body_two, 'inner')\n",
    "\n",
    "i2s = IndexToString(inputCol=\"prediction\", outputCol=\"tag\", labels=indexer.labels)\n",
    "output = i2s.transform(output)\n",
    "\n",
    "output = output[['id', 'prediction', 'tag', 'predicted_labels', 'labels_decoder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7461461e-b2f6-4a3b-81dc-8d5dd1ddfb1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1058ea95-7618-4a3d-aa7f-d28bb9299511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "save = output[['id', 'prediction', 'tag']]\n",
    "\n",
    "\n",
    "# define this function\n",
    "def crt_sgl_file(result_path):\n",
    "        # write the result to a folder container several files\n",
    "        path = \"/mnt/bd-project/Predictions/Temp_Parq/\"\n",
    "        save.write.option(\"delimiter\", \",\").option(\"header\", \"true\").mode(\"overwrite\").csv(path)\n",
    "\n",
    "        # list the folder, find the csv file \n",
    "        filenames = dbutils.fs.ls(path)\n",
    "        name = ''\n",
    "        for filename in filenames:\n",
    "            if filename.name.endswith('csv'):\n",
    "                org_name = filename.name\n",
    "\n",
    "        # copy the csv file to the path you want to save, in this example, we use  \"/mnt/deBDProject/BI/ml_result.csv\"\n",
    "        dbutils.fs.cp(path + '/'+ org_name, result_path)\n",
    "\n",
    "        # delete the folder\n",
    "        dbutils.fs.rm(path, True)\n",
    "\n",
    "        print('single file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e8a34f-915e-4f0f-ae32-01ded0b8ea49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# run the function\n",
    "result_path = \"/mnt/bd-project/Predictions/predictions_{}.csv\".format(timestamp)\n",
    "\n",
    "crt_sgl_file(result_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5_Model_Prediction",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
